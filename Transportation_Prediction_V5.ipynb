{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 4007)\n",
      "/home/ai2-jedi/anaconda2/lib/python2.7/site-packages/pandas/io/data.py:33: FutureWarning: \n",
      "The pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\n",
      "After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n",
      "  FutureWarning)\n",
      "/home/ai2-jedi/anaconda2/lib/python2.7/site-packages/pandas/io/wb.py:19: FutureWarning: \n",
      "The pandas.io.wb module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\n",
      "After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n",
      "  FutureWarning)\n",
      "/home/ai2-jedi/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Activation, Dropout, TimeDistributedDense,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD,Adadelta,RMSprop\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Merge\n",
    "import pandas as pd  \n",
    "from random import random\n",
    "import numpy as np\n",
    "import talib as ta\n",
    "from datetime import datetime\n",
    "from pandas.io import data, wb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import urllib2\n",
    "import pytz\n",
    "from bs4 import BeautifulSoup\n",
    "import copy\n",
    "import csv\n",
    "import urllib2\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset = []\n",
    "with open('User_085_Mode_Transport_Medium_Cluster_v2.csv','rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for row in spamreader:\n",
    "        dset.append([row[3],row[4],row[5],row[7],row[8],row[9],row[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5.0': 46, '1.0': 1073, '4.0': 79, '800.0': 11, '0.0': 12373, '3.0': 11390, '6.0': 28}\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "dset = dset[1:]\n",
    "for row in dset:\n",
    "    #print row[6]\n",
    "    if row[6] not in clusters.keys():\n",
    "        clusters[row[6]] = 1\n",
    "    else:\n",
    "        clusters[row[6]] = clusters[row[6]] + 1\n",
    "print clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(dset)):\n",
    "    val = dset[row][6]\n",
    "    if val != '0.0' and dset[row][6] != '3.0':\n",
    "        dset[row][6] = '2.0'\n",
    "        \n",
    "for row in range(len(dset)):\n",
    "    if dset[row][6] == '3.0':\n",
    "        dset[row][6] = '1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.0': 12373, '2.0': 1237, '1.0': 11390}\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "for row in dset:\n",
    "    if row[6] not in clusters.keys():\n",
    "        clusters[row[6]] = 1\n",
    "    else:\n",
    "        clusters[row[6]] = clusters[row[6]] + 1\n",
    "print clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['39.9762', '116.33032', '2.48579225103', 'walk', '13', '56', '0.0']\n",
      "['39.97619', '116.33032', '0.32437826823', 'walk', '13', '56', '0.0']\n",
      "25000\n",
      "{'taxi': 380, 'bus': 5568, 'walk': 19052}\n"
     ]
    }
   ],
   "source": [
    "print dset[0]\n",
    "print dset[1]\n",
    "#dset = dset[1:]\n",
    "print (len(dset))\n",
    "modes = {}\n",
    "for row in dset:\n",
    "    if row[3] not in modes.keys():\n",
    "        modes[row[3]] = 1\n",
    "    else:\n",
    "        modes[row[3]] = modes[row[3]] + 1\n",
    "print modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'taxi': 0, 'bus': 1, 'walk': 2}\n"
     ]
    }
   ],
   "source": [
    "classes = {}\n",
    "for (i,j) in enumerate(modes.keys()):\n",
    "    classes[j] = i\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.9762, 116.33032, 2.48579225103, 2, 13.0, 56.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(dset)):\n",
    "    dset[row][3] = int(classes[dset[row][3]])\n",
    "    dset[row][0] = float(dset[row][0])\n",
    "    dset[row][1] = float(dset[row][1])\n",
    "    dset[row][2] = float(dset[row][2])\n",
    "    dset[row][4] = float(dset[row][4])\n",
    "    dset[row][5] = float(dset[row][5])\n",
    "    dset[row][6] = float(dset[row][6])\n",
    "print(dset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 360 #45 Mins\n",
    "N_HIDDEN = 100\n",
    "NUM_CLASSES = 3\n",
    "NUM_ATTR = 6\n",
    "LOOK_FORWARD = 360 #15\n",
    "\n",
    "def createSeqData(data, n_prev = SEQ_LEN):\n",
    "    docX, docY = [], []\n",
    "    num_sequences = (len(data)-n_prev - LOOK_FORWARD)\n",
    "    for i in range(num_sequences):\n",
    "        x = (data.iloc[i:i+n_prev,:NUM_ATTR].as_matrix())\n",
    "        y = (data.iloc[i+n_prev + LOOK_FORWARD,NUM_ATTR:].as_matrix())        \n",
    "        docX.append(x)\n",
    "        docY.append(y)\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "    return alsX, alsY\n",
    "\n",
    "\n",
    "def splitDset(dset, ratio):\n",
    "    index = int(round(ratio*len(dset)))\n",
    "    #print (index)\n",
    "    train_dset = dset[:index]\n",
    "    test_dset = dset[index:]\n",
    "    return train_dset, test_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16267, 360, 6)\n",
      "(16267, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.asarray(dset))\n",
    "X, y = createSeqData(df)\n",
    "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=23)\n",
    "b_train = b_train.astype(int)\n",
    "b_train = to_categorical(b_train)\n",
    "print(a_train.shape)\n",
    "print(b_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  39.9396      116.30452       4.49063971    2.            9.           16.        ]\n",
      " [  39.93951     116.30451       2.51485211    2.            9.           16.        ]\n",
      " [  39.93946     116.3045        3.37787796    2.            9.           16.        ]\n",
      " ..., \n",
      " [  39.97012     116.3006        0.38101329    2.            9.           47.        ]\n",
      " [  39.97012     116.30059       0.            2.            9.           47.        ]\n",
      " [  39.97012     116.30059       0.38101268    2.            9.           48.        ]]\n",
      "[ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print (a_train[0])\n",
    "print (b_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_intra = Sequential() \n",
    "model_intra.add(LSTM(N_HIDDEN, return_sequences=True, activation='tanh', input_shape=(SEQ_LEN, NUM_ATTR)))\n",
    "model_intra.add(LSTM(N_HIDDEN, return_sequences=False, activation='tanh'))\n",
    "model_intra.add(Dense(3, activation='softmax'))\n",
    "model_intra.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13013 samples, validate on 3254 samples\n",
      "Epoch 1/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.7655 - acc: 0.5990 - val_loss: 0.6542 - val_acc: 0.7099\n",
      "Epoch 2/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.6257 - acc: 0.7187 - val_loss: 0.5656 - val_acc: 0.7566\n",
      "Epoch 3/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.5494 - acc: 0.7543 - val_loss: 0.6308 - val_acc: 0.6853\n",
      "Epoch 4/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.5012 - acc: 0.7893 - val_loss: 0.4394 - val_acc: 0.8236\n",
      "Epoch 5/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.4864 - acc: 0.7981 - val_loss: 0.6110 - val_acc: 0.6998\n",
      "Epoch 6/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.4763 - acc: 0.7990 - val_loss: 0.3391 - val_acc: 0.8682\n",
      "Epoch 7/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.4185 - acc: 0.8349 - val_loss: 0.3222 - val_acc: 0.8672\n",
      "Epoch 8/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.3853 - acc: 0.8454 - val_loss: 0.2560 - val_acc: 0.9127\n",
      "Epoch 9/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.3171 - acc: 0.8820 - val_loss: 0.3996 - val_acc: 0.8316\n",
      "Epoch 10/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.2865 - acc: 0.8891 - val_loss: 0.2805 - val_acc: 0.8964\n",
      "Epoch 11/15\n",
      "13013/13013 [==============================] - 38s - loss: 0.2798 - acc: 0.9016 - val_loss: 0.3744 - val_acc: 0.8657\n",
      "Epoch 12/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.2098 - acc: 0.9265 - val_loss: 0.2018 - val_acc: 0.9302\n",
      "Epoch 13/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.2595 - acc: 0.9033 - val_loss: 0.2919 - val_acc: 0.8841\n",
      "Epoch 14/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.2307 - acc: 0.9138 - val_loss: 0.1454 - val_acc: 0.9508\n",
      "Epoch 15/15\n",
      "13013/13013 [==============================] - 37s - loss: 0.2439 - acc: 0.9137 - val_loss: 0.4107 - val_acc: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa056768b50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_intra.fit(a_train, b_train, batch_size=250, nb_epoch=15, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8013\n",
      "8013\n"
     ]
    }
   ],
   "source": [
    "print(len(a_test))\n",
    "print(len(b_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model_intra.predict(a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836515662049\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(Y_pred)):\n",
    "    if Y_pred[i].argmax() == b_test[i]:\n",
    "        count = count + 1\n",
    "        \n",
    "acc = count/float(len(b_test))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "(3007, 3997, 0.7523142356767576)\n",
      "Cluster 2:\n",
      "(3417, 3726, 0.9170692431561996)\n",
      "Cluster 3(Other):\n",
      "(279, 290, 0.9620689655172414)\n",
      "Total:\n",
      "0.836515662049\n"
     ]
    }
   ],
   "source": [
    "cluster1_total = 0\n",
    "cluster1_correct = 0\n",
    "cluster2_count = 0\n",
    "cluster2_correct = 0\n",
    "cluster3_total = 0\n",
    "cluster3_correct = 0\n",
    "\n",
    "\n",
    "for i in range(len(Y_pred)):\n",
    "\n",
    "        \n",
    "    if b_test[i] == 0:\n",
    "        cluster1_total = cluster1_total + 1\n",
    "        \n",
    "        if Y_pred[i].argmax() == b_test[i]:\n",
    "            cluster1_correct = cluster1_correct + 1\n",
    "            \n",
    "    if b_test[i] == 1:\n",
    "        cluster2_count = cluster2_count + 1\n",
    "        \n",
    "        if Y_pred[i].argmax() == b_test[i]:\n",
    "            cluster2_correct = cluster2_correct + 1\n",
    "            \n",
    "    if b_test[i] == 2:\n",
    "        cluster3_total = cluster3_total + 1\n",
    "        \n",
    "        if Y_pred[i].argmax() == b_test[i]:\n",
    "            cluster3_correct = cluster3_correct + 1\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "cluster1_acc = cluster1_correct/float(cluster1_total)\n",
    "cluster2_acc = cluster2_correct/float(cluster2_count)\n",
    "print ('Cluster 1:')\n",
    "print(cluster1_correct, cluster1_total, cluster1_acc)\n",
    "\n",
    "print ('Cluster 2:')\n",
    "print(cluster2_correct, cluster2_count, cluster2_acc)\n",
    "\n",
    "cluster3_acc = cluster3_correct/float(cluster3_total)\n",
    "print ('Cluster 3(Other):')\n",
    "print(cluster3_correct, cluster3_total, cluster3_acc)\n",
    "\n",
    "\n",
    "print ('Total:')\n",
    "total_correct = cluster1_correct + cluster2_correct + cluster3_correct\n",
    "acc = total_correct/float(len(Y_pred))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
